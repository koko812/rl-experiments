# LEARNED.md v0.2 — Rule-based Control Completion and Toward Q-Learning

## 🧜‍♂️ 現在地：ルールベース制御の確立と可視化の完成

CartPole 環境における人間操作・ランダムエージェント・複数のルールベースエージェントによる比較を通じて、
強化学習以前の基礎的制御の検証と可視化が完了した。

---

## 📂 ディレクトリ構造と設計思想

```plaintext
rl-experiment/
├── agents/                 # 各種エージェント（rule-based, Q-learning など）
│   ├── base_agent.py
│   ├── rule_agent.py
│   └── q_learning_agent.py  # ※ 次フェーズで導入予定
├── scripts/                # 実行スクリプト（ログ収集・可視化・学習）
│   ├── run_cartpole_log.py
│   ├── plot_cartpole_log.py
│   └── train_cartpole_q.py  # ※ Q学習導入後に追加予定
├── logs/                   # ログ保存先（cartpole 環境用）
│   └── cartpole/
│       ├── human_play_log.json
│       ├── simple_log.json
│       ├── angle_only_log.json
│       └── predictive_log.json
├── LEARNED.md              # 学習ログと進捗記録
└── README.md               # プロジェクト概要
```

### 🌐 設計思想：拡張性と比較可能性の確保

* **エージェントはクラス単位で独立**し、共通の `select_action(obs)` インターフェースを実装。
* **スクリプトは処理単位（ログ出力・可視化・学習）で分離**し、目的ごとの簡易実行が可能。
* **ログは agent 単位に命名保存**し、再現・比較・可視化が容易。
* 今後 DQN や LunarLander 等に拡張しても構造が壊れず、**最低限の変更で多環境対応可能な設計**となっている。

---

## ✅ 実装・試行内容

### 1. 🌎 Human Play Logging

* Pygame を用いて左右キー操作でプレイ可能な CartPole UI を構築。
* `logs/cartpole/human_play_log.json` に 500 ステップまでのプレイログを保存。
* `plot_cartpole_log.py` の修正により任意のログファイルを可視化可能に。

### 2. 🤷 Rule-based Agent の追加と比較

* `AngleOnlyAgent`：obs\[2]（pole\_angle）の正負によって左右に操作。
* `SimpleRuleAgent`（当初）：`AngleOnlyAgent` と同等の制御であったため、差別化は保留。
* `PredictiveAngleAgent`：`angle + 0.5 * angle_velocity` に基づき予測的な制御を導入。

### 3. 📈 可視化基盤の進化

* `--log` 引数で任意のログファイルを指定可能に。
* 存在しないログファイルを指定した場合は候補一覧を表示。
* 続續報酬のプロットにより、各エージェントの成績を直感的に比較可能に。

### 4. ✨ 成果

* `PredictiveAngleAgent` によって CartPole を 500 ステップ維持（報酬満点）達成。
* 観測グラフ上でも角度の揺れが安定して抑えられており、簡単な予測モデルでも十分な効果を発揮することが示された。

---

## 🔄 次のステップ：Q-Learning の導入へ

* 離散化による状態表現と Q テーブルによる価値更新の実装へ移行予定。
* `BaseAgent` の設計が統一されているため、`QLearningAgent` の導入も自然に行える。
* 将来的には DQN を含むニューラル制御型との比較基盤にも拡張可能。
* その過程で、カート位置が中心に近いにつれボーナスを追加するような報酬設計の実験も検討中。
* 現在の Gym のデフォルト報酬 (1.0) に加算して
  `reward += α * (1.0 - abs(cart_position) / 2.4)`
  とするなどの実装を通じて、左右に振り切られるより「中心に戻ろうとする行動」を評価する方向へのシフトを目指す。

---

## 🔍 学習ログと今後の観察点

* 予測型ルールが環境に極めて強いことが判明 → Q 学習でこれを超えられるか？
* 学習曲線・探索率・状態空間の分割方法などの設計が、今後の性能に大きく関与する見通し。
* ルールベースの限界と、学習ベースの適応性との比較を無理なく観察を続ける。

---

（v0.2 完）


</br>
</br>
</br>
</br>
</br>

# LEARNED.md v0.1 — 強化学習入門：CartPole を題材にした可視化と分析

## 🧠 背景

Stack Tower ゲームを強化学習で制御するには、状態・行動・報酬が複雑すぎるため、まずは OpenAI Gym の CartPole 環境を用いたシンプルな強化学習の理解と分析に取り組むことにした。

---

## ✅ 段階的な取り組み

### 1. CartPole のランダムプレイログ収集

- `gymnasium.make("CartPole-v1")` を使用
- ランダムな行動で500ステップ程度プレイし、`obs`, `action`, `reward`, `done` を JSON に保存
- 出力形式は `json` と `csv` に切り替え可能な設計にしておき、分析に応じて使い分けられるようにした

### 2. 観測ログの可視化（plot_cartpole_log.py）

- 左グラフ：ポール角度 (`obs[2]`) の時間変化（赤線は `done=True` の瞬間）
- 右グラフ：報酬の推移を可視化（最初はステップごとの `reward`）

### 3. エピソードごとの累積報酬の算出

- `done=True` を区切りとし、各エピソードで `reward` を合計
- エピソード単位での報酬の伸び方・変動を分析できるようにした

### 4. ステップ単位での「エピソード内累積報酬」へ変更

- より直感的に「どこでゲームオーバーになったか」を確認できるように修正
- `done=True` のたびに累積報酬をリセットすることで、グラフ上で階段のようなパターンが見えるようになった

---

## 📊 分析のための工夫

- `Path(__file__).resolve().parent.parent` を使って、どこから実行してもログが読めるようにパスを統一
- `done=True` の位置に `axvline` で可視的な区切りを入れることで、エピソードの構造が見やすくなった
- JSON ログには `step` や `obs`, `reward` の全履歴が含まれており、後からあらゆる可視化に再利用できる

---

## 🎯 現時点での知見・気づき

- `done=True` のタイミングを意識すると、ポールがどれだけ揺れていると倒れるかがだんだん見えてくる
- エピソードの累積報酬のグラフは、**学習が進んでいるかどうか**の評価にも直感的に使える
- 今後、ルールベースや学習エージェントと比較する際にもこのログがベースになる
- 分析や可視化の柔軟性を保つためにも、ログ出力は汎用的にしておくべき

---

## 🔜 次のステップ案

- ルールベース（if-else）エージェントを作って、ランダムと比較
- 学習エージェント（Q-Learning / DQN）導入前に、報酬設計・行動空間の理解を整理
- ログを `notebooks/analysis.ipynb` でより詳細に可視化
- CartPole の他に LunarLander も同様のログ化・可視化基盤に乗せる
